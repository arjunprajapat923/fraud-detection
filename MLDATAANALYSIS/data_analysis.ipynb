{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanush005/100-days-of-deep-learning/blob/main/data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J2U31yITDS_"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow numpy pandas scikit-learn matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hFlw-OwxT3A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "zbUlR0yYT5mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy pandas scikit-learn imbalanced-learn xgboost matplotlib seaborn"
      ],
      "metadata": {
        "id": "A7UNaKhnUFnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('synthetic_eth_transactions.csv')\n",
        "\n",
        "# Display dataset info\n",
        "print(data.info())\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "F8dZSViZUH3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Load transaction data (Assuming a CSV file)\n",
        "df = pd.read_csv(\"/content/synthetic_ethereum_transactions (1).csv\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Overview:\\n\")\n",
        "print(df.info())  # Check data types & missing values\n",
        "print(\"\\nSummary Statistics:\\n\", df.describe())\n",
        "\n",
        "# Checking for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"\\nMissing Values:\\n\", missing_values[missing_values > 0])\n",
        "\n",
        "# Checking for duplicate transactions\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nNumber of Duplicate Records: {duplicates}\")\n",
        "\n",
        "\n",
        "# Convert 'timestamp' to datetime (if applicable)\n",
        "if 'timestamp' in df.columns:\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.sort_values(by='timestamp')  # Sort transactions chronologically\n",
        "\n",
        "# Convert categorical features (if needed)\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_cols] = df[categorical_cols].astype('category')\n",
        "\n",
        "# Handling outliers using IQR method for transaction amounts\n",
        "if 'amount' in df.columns:\n",
        "    Q1 = df['amount'].quantile(0.25)\n",
        "    Q3 = df['amount'].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    df = df[(df['amount'] >= lower_bound) & (df['amount'] <= upper_bound)]  # Remove outliers\n",
        "\n",
        "\n",
        "\n",
        "# 1. Transaction Amount Distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Amount'], bins=50, kde=True, color='blue')\n",
        "plt.title('Transaction Amount Distribution')\n",
        "plt.xlabel('Transaction Amount')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# 2. Frequency of Transactions Per Day\n",
        "if 'timestamp' in df.columns:\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    daily_txn_counts = df.groupby('date').size()\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.lineplot(x=daily_txn_counts.index, y=daily_txn_counts.values, marker=\"o\")\n",
        "    plt.title('Daily Transaction Frequency')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Transactions')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# 3. Time Intervals Between Transactions\n",
        "if 'timestamp' in df.columns:\n",
        "    df['time_diff'] = df['timestamp'].diff().dt.total_seconds()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(df['time_diff'].dropna(), bins=50, kde=True, color='green')\n",
        "    plt.title('Distribution of Time Intervals Between Transactions')\n",
        "    plt.xlabel('Time Interval (seconds)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "1v03K4QDft25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/synthetic_ethereum_transactions (1).csv')\n",
        "\n",
        "# Function to shorten addresses\n",
        "def shorten_address(addr):\n",
        "    return addr[:4] + \"...\" + addr[-4:] if len(addr) > 10 else addr\n",
        "\n",
        "# Count the number of transactions per sender and receiver\n",
        "top_senders = df['Sender'].value_counts().head(10).reset_index()\n",
        "top_senders.columns = ['Sender', 'Transactions']\n",
        "\n",
        "top_receivers = df['Receiver'].value_counts().head(10).reset_index()\n",
        "top_receivers.columns = ['Receiver', 'Transactions']\n",
        "\n",
        "# Apply function to sender & receiver labels\n",
        "top_senders[\"Short_Sender\"] = top_senders[\"Sender\"].apply(shorten_address)\n",
        "top_receivers[\"Short_Receiver\"] = top_receivers[\"Receiver\"].apply(shorten_address)\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Pie chart for top senders\n",
        "ax[0].pie(top_senders['Transactions'], labels=top_senders['Short_Sender'], autopct='%1.1f%%',\n",
        "          startangle=140, colors=plt.cm.coolwarm(range(10)), textprops={'fontsize': 9})\n",
        "ax[0].set_title(\"Top 10 Senders (Pie Chart)\")\n",
        "\n",
        "# Pie chart for top receivers\n",
        "ax[1].pie(top_receivers['Transactions'], labels=top_receivers['Short_Receiver'], autopct='%1.1f%%',\n",
        "          startangle=140, colors=plt.cm.viridis(range(10)), textprops={'fontsize': 9})\n",
        "ax[1].set_title(\"Top 10 Receivers (Pie Chart)\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3c9OpALRrK4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"synthetic_eth_transactions.csv\")\n",
        "\n",
        "# Ensure timestamp is in datetime format\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "# ------------------------------\n",
        "# Feature Engineering\n",
        "# ------------------------------\n",
        "\n",
        "# 1. **Transaction Amount Features**\n",
        "df['log_amount'] = np.log1p(df['amount'])  # Log-transform to reduce skewness\n",
        "df['large_txn_flag'] = (df['amount'] > df['amount'].quantile(0.99)).astype(int)  # Flag top 1% transactions\n",
        "\n",
        "# 2. **Frequency-Based Features**\n",
        "df['daily_txn_count'] = df.groupby(['sender', df['timestamp'].dt.date])['transaction_id'].transform('count')\n",
        "df['hourly_txn_count'] = df.groupby(['sender', df['timestamp'].dt.hour])['transaction_id'].transform('count')\n",
        "\n",
        "# # 3. **Time-Based Features**\n",
        "# df['is_night_txn'] = df['hour_of_day'].apply(lambda x: 1 if x < 6 or x > 22 else 0)  # Transactions between 10 PM - 6 AM\n",
        "# df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x in [5, 6] else 0)  # Weekend transactions\n",
        "\n",
        "# 4. **Transaction Speed Features**\n",
        "df['txn_speed'] = df.groupby('sender')['timestamp'].diff().dt.total_seconds()  # Time between transactions\n",
        "df['avg_txn_speed'] = df.groupby('sender')['txn_speed'].transform('mean')  # Average transaction speed per sender\n",
        "\n",
        "# # 5. **Gas Usage & Transaction Fee Features**\n",
        "# df['effective_gas_price'] = df['gas_price'] * df['gas_used']\n",
        "# df['relative_gas_usage'] = df['gas_used'] / df['gas_used'].max()\n",
        "# df['txn_fee_to_amount_ratio'] = df['transaction_fee'] / (df['amount'] + 1)  # Normalized fee-to-amount ratio\n",
        "\n",
        "# 6. **Behavioral Features (Deviation from Historical Patterns)**\n",
        "df['sender_avg_txn_amt'] = df.groupby('sender')['amount'].transform('mean')\n",
        "df['sender_txn_amt_std'] = df.groupby('sender')['amount'].transform('std')\n",
        "df['txn_amt_deviation'] = (df['amount'] - df['sender_avg_txn_amt']) / (df['sender_txn_amt_std'] + 1e-9)  # Z-score\n",
        "\n",
        "# 7. **Network Features**\n",
        "df['sender_txn_count'] = df.groupby('sender')['transaction_id'].transform('count')  # How many transactions has the sender made?\n",
        "df['receiver_txn_count'] = df.groupby('receiver')['transaction_id'].transform('count')  # Transactions received\n",
        "df['unique_receivers'] = df.groupby('sender')['receiver'].transform('nunique')  # How many unique addresses a sender has sent money to\n",
        "df['unique_senders'] = df.groupby('receiver')['sender'].transform('nunique')  # Unique senders\n",
        "\n",
        "# 8. **Anomaly Detection Flags**\n",
        "df['high_freq_flag'] = (df['daily_txn_count'] > df['daily_txn_count'].quantile(0.99)).astype(int)  # Unusually frequent transactions\n",
        "# df['high_txn_fee_flag'] = (df['txn_fee_to_amount_ratio'] > df['txn_fee_to_amount_ratio'].quantile(0.99)).astype(int)  # High fee transactions\n",
        "df['behavioral_anomaly_flag'] = (df['txn_amt_deviation'].abs() > 3).astype(int)  # Transactions outside 3-sigma range\n",
        "\n",
        "# Drop unnecessary intermediate columns if needed\n",
        "df.drop(columns=['txn_speed'], inplace=True)\n",
        "\n",
        "# Save the engineered dataset\n",
        "df.to_csv(\"transaction_data_featured.csv\", index=False)\n",
        "\n",
        "print(\"Feature Engineering Complete. Saved as 'transaction_data_featured.csv'.\")\n"
      ],
      "metadata": {
        "id": "yVINzYtift5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the engineered dataset\n",
        "df = pd.read_csv(\"/content/transaction_data_featured.csv\")\n",
        "\n",
        "# Set up the style for plots\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "\n",
        "# Log-transformed amount distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['log_amount'], bins=50, kde=True, color='blue')\n",
        "plt.title('Distribution of Log-Transformed Transaction Amounts')\n",
        "plt.xlabel('Log(Amount)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Large transaction flag distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='large_txn_flag', data=df, palette='Set2')\n",
        "plt.title('Distribution of Large Transaction Flags')\n",
        "plt.xlabel('Large Transaction Flag (1 = Large)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Daily transaction count distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['daily_txn_count'], bins=50, kde=True, color='green')\n",
        "plt.title('Distribution of Daily Transaction Counts')\n",
        "plt.xlabel('Daily Transaction Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "LEg0aDHQ7arm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of Transaction Amount (Before & After Log Transformation)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.histplot(df['amount'], bins=50, ax=ax[0], kde=True)\n",
        "ax[0].set_title(\"Original Transaction Amounts\")\n",
        "\n",
        "sns.histplot(df['log_amount'], bins=50, ax=ax[1], kde=True)\n",
        "ax[1].set_title(\"Log Transformed Transaction Amounts\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SDrRuHyPft70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mF8WBmTyft-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wzHbLoHRduuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"synthetic_eth_transactions.csv\")\n",
        "\n",
        "# Select numeric features for clustering & anomaly detection\n",
        "numeric_features = df.select_dtypes(include=['number'])\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(numeric_features)\n",
        "\n",
        "## K-Means Clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "df['kmeans_cluster'] = kmeans.fit_predict(scaled_data)\n",
        "\n",
        "## DBSCAN Clustering (Detects small outlier clusters)\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "df['dbscan_cluster'] = dbscan.fit_predict(scaled_data)\n",
        "\n",
        "\n",
        "## Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
        "df['iso_forest_score'] = iso_forest.fit_predict(scaled_data)\n",
        "\n",
        "## One-Class SVM\n",
        "oc_svm = OneClassSVM(nu=0.01, kernel=\"rbf\", gamma='auto')\n",
        "df['oc_svm_score'] = oc_svm.fit_predict(scaled_data)\n",
        "\n",
        "\n",
        "input_dim = scaled_data.shape[1]\n",
        "\n",
        "\n",
        "autoencoder = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(input_dim,), kernel_initializer='he_normal'),\n",
        "    Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dense(8, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dense(16, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dense(32, activation='relu', kernel_initializer='he_normal'),\n",
        "    Dense(input_dim, activation='linear')  # Reconstruction\n",
        "])\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = autoencoder.fit(scaled_data, scaled_data, epochs=10, batch_size=32, shuffle=True, verbose=0, callbacks=[LossHistory(), early_stopping])\n",
        "\n",
        "history = autoencoder.fit(scaled_data, scaled_data,epochs=10, batch_size=32, shuffle=True, verbose=0,callbacks=[LossHistory(), early_stopping])\n",
        "\n",
        "\n",
        "# Compute reconstruction error\n",
        "reconstructed = autoencoder.predict(scaled_data)\n",
        "reconstruction_error = np.mean(np.abs(reconstructed - scaled_data), axis=1)\n",
        "df['autoencoder_score'] = reconstruction_error\n",
        "\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    G.add_edge(row['sender'], row['receiver'], weight=row['amount'])\n",
        "\n",
        "# Compute Node Features (PageRank & Betweenness Centrality)\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# Check if the graph is weakly connected (ignoring edge directions)\n",
        "if not nx.is_weakly_connected(G):\n",
        "    print(\"Warning: Graph is disconnected! Computing centrality on the largest weakly connected component.\")\n",
        "\n",
        "    # Get the largest weakly connected component and convert it back to a DiGraph\n",
        "    largest_component = max(nx.weakly_connected_components(G), key=len)\n",
        "    G = G.subgraph(largest_component).copy()\n",
        "\n",
        "# Compute Betweenness Centrality (Optimized for Large Graphs)\n",
        "import networkx as nx\n",
        "from joblib import Parallel, delayed\n",
        "import numpy as np\n",
        "\n",
        "def compute_betweenness(G, node_subset):\n",
        "    \"\"\"Compute betweenness centrality for a subset of nodes.\"\"\"\n",
        "    return nx.betweenness_centrality(G, k=None, normalized=True, endpoints=True)\n",
        "\n",
        "# Ensure the graph is connected\n",
        "if not nx.is_connected(G.to_undirected()):\n",
        "    print(\"Warning: Graph is disconnected! Computing on the largest component.\")\n",
        "    G = G.subgraph(max(nx.connected_components(G.to_undirected()), key=len))\n",
        "\n",
        "# Split nodes for parallel computation\n",
        "nodes = list(G.nodes())\n",
        "num_chunks = 4  # Adjust based on your system's CPU\n",
        "chunks = np.array_split(nodes, num_chunks)\n",
        "\n",
        "# Run in parallel\n",
        "betweenness_results = Parallel(n_jobs=num_chunks)(\n",
        "    delayed(compute_betweenness)(G, chunk) for chunk in chunks\n",
        ")\n",
        "\n",
        "# Merge results\n",
        "betweenness_centrality = {node: 0 for node in nodes}\n",
        "for result in betweenness_results:\n",
        "    for node, value in result.items():\n",
        "        betweenness_centrality[node] += value\n",
        "\n",
        "print(\" Parallel Betweenness Centrality Computed Successfully!\")\n",
        "\n",
        "\n",
        "betweenness = {node: sum(res.get(node, 0) for res in betweenness_results) for node in G.nodes()}\n",
        "\n",
        "\n",
        "print(\"Betweenness centrality computed successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pagerank = nx.pagerank(G, alpha=0.85)\n",
        "\n",
        "df['pagerank'] = df['sender'].map(pagerank)\n",
        "df['betweenness'] = df['sender'].map(betweenness)\n",
        "\n",
        "\n",
        "\n",
        "# Handle missing values for unseen senders\n",
        "df.fillna({'pagerank': 0, 'betweenness': 0}, inplace=True)\n",
        "\n",
        "# ---------- 5. Hybrid Anomaly Score ----------\n",
        "df['anomaly_score'] = (\n",
        "    (df['iso_forest_score'] == -1).astype(int) +\n",
        "    (df['oc_svm_score'] == -1).astype(int) +\n",
        "    (df['autoencoder_score'] > df['autoencoder_score'].quantile(0.99)).astype(int) +\n",
        "    (df['pagerank'] > df['pagerank'].quantile(0.99)).astype(int) +\n",
        "    (df['betweenness'] > df['betweenness'].quantile(0.99)).astype(int) +\n",
        "    (df['dbscan_cluster'] == -1).astype(int)\n",
        ")\n",
        "\n",
        "df['fraud_flag'] = (df['anomaly_score'] > 2).astype(int)  # Threshold for fraud detection\n",
        "\n",
        "# ---------- 6. Visualize Results ----------\n",
        "sns.histplot(df['anomaly_score'], bins=20, kde=True)\n",
        "plt.title(\"Anomaly Score Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Output top 10 suspected fraudulent transactions\n",
        "fraudulent_transactions = df[df['fraud_flag'] == 1]\n",
        "print(fraudulent_transactions[['transaction_id', 'sender', 'receiver', 'amount', 'fraud_flag']].head(10))\n"
      ],
      "metadata": {
        "id": "h9Yo9l3tfuBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio \\\n",
        "            torch-geometric \\\n",
        "            networkx numpy pandas gym stable-baselines3 \\\n",
        "            scikit-learn matplotlib\n",
        "\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "            -f https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
        "!pip install \"shimmy>=2.0\"\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gym\n",
        "import torch.nn.functional as F\n",
        "from gym import spaces\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from stable_baselines3 import PPO\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.decomposition import PCA\n",
        "# Load dataset\n",
        "file_path = \"ss.csv\"\n",
        "if not os.path.exists(file_path):\n",
        "    print(\" Uploading dataset. Please select your file.\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    file_path = list(uploaded.keys())[0]\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check necessary columns\n",
        "expected_cols = ['Sender', 'Receiver', 'Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']\n",
        "missing_cols = [col for col in expected_cols if col not in df.columns]\n",
        "if missing_cols:\n",
        "    raise KeyError(f\"Missing columns in dataset: {missing_cols}\")\n",
        "\n",
        "# Normalize transaction features\n",
        "scaler = MinMaxScaler()\n",
        "df[['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']] = scaler.fit_transform(\n",
        "    df[['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']]\n",
        ")\n",
        "\n",
        "# Create node index mapping\n",
        "unique_addresses = set(df['Sender'].astype(str).tolist() + df['Receiver'].astype(str).tolist())\n",
        "address_map = {addr: i for i, addr in enumerate(unique_addresses)}\n",
        "\n",
        "df['sender_idx'] = df['Sender'].astype(str).map(address_map)\n",
        "df['receiver_idx'] = df['Receiver'].astype(str).map(address_map)\n",
        "\n",
        "# Prepare LSTM Data (Sequential Patterns)\n",
        "def prepare_lstm_data(df, seq_length=10):\n",
        "    sequences = []\n",
        "    for i in range(len(df) - seq_length):\n",
        "        seq = df.iloc[i:i+seq_length][['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].values\n",
        "        sequences.append(seq)\n",
        "    return torch.tensor(sequences, dtype=torch.float32)\n",
        "\n",
        "lstm_data = prepare_lstm_data(df)\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMFraudDetector(nn.Module):\n",
        "    def _init_(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMFraudDetector, self)._init_()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        return torch.tanh(self.fc(hn[-1]))  # Output anomaly score\n",
        "\n",
        "lstm_model = LSTMFraudDetector(input_dim=4, hidden_dim=64, output_dim=1)\n",
        "\n",
        "# Prepare Graph Data\n",
        "graph = nx.DiGraph()\n",
        "for _, row in df.iterrows():\n",
        "    graph.add_edge(row['sender_idx'], row['receiver_idx'], weight=row['Amount'])\n",
        "\n",
        "graph_data = Data(\n",
        "    edge_index=torch.tensor(list(graph.edges)).t().contiguous(),\n",
        "    edge_attr=torch.tensor([graph[u][v]['weight'] for u, v in graph.edges], dtype=torch.float32)\n",
        ")\n",
        "\n",
        "\n",
        "class GNNFraudDetector(nn.Module):\n",
        "    def _init_(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNNFraudDetector, self)._init_()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, node_features, edge_index):\n",
        "        \"\"\" Forward pass with correct node features. \"\"\"\n",
        "        x = self.conv1(node_features, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "    def get_node_embeddings(self, node_features, edge_index):\n",
        "       with torch.no_grad():\n",
        "          x = self.conv1(node_features, edge_index)\n",
        "          x = F.relu(x)\n",
        "          embeddings = self.conv2(x, edge_index)\n",
        "       return embeddings\n",
        "\n",
        "gnn_model = GNNFraudDetector(input_dim=4, hidden_dim=64, output_dim=64)\n",
        "\n",
        "# Define RL Environment\n",
        "class FraudDetectionEnv(gym.Env):\n",
        "    def _init_(self, df, gnn_model, lstm_model):\n",
        "        super(FraudDetectionEnv, self)._init_()\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=-1, high=1, shape=(65,), dtype=np.float32)\n",
        "\n",
        "        self.df = df\n",
        "        self.gnn_model = gnn_model\n",
        "        self.lstm_model = lstm_model\n",
        "        self.current_index = 0\n",
        "\n",
        "    def reset(self):\n",
        "      self.current_index = 0\n",
        "      return self._get_features(self.df.iloc[self.current_index])\n",
        "\n",
        "    def _get_features(self, transaction):\n",
        "      if isinstance(transaction, pd.DataFrame):\n",
        "        transaction = transaction.iloc[0]  # Convert DataFrame to Series\n",
        "\n",
        "      sender_idx = int(transaction['sender_idx'])\n",
        "      receiver_idx = int(transaction['receiver_idx'])\n",
        "\n",
        "\n",
        "      edge_index = torch.tensor([[sender_idx], [receiver_idx]],\n",
        "                                dtype=torch.long)\n",
        "\n",
        "      edge_attr = torch.tensor([float(transaction['Amount'])], dtype=torch.float32)\n",
        "\n",
        "      num_nodes = max(sender_idx, receiver_idx) + 1\n",
        "      node_features = torch.randn(num_nodes, 4)\n",
        "\n",
        "      gnn_embeddings = self.gnn_model(node_features,edge_index )\n",
        "      if sender_idx >= gnn_embeddings.shape[0]:\n",
        "        sender_idx = 0\n",
        "\n",
        "      gnn_embedding1 = gnn_embeddings[sender_idx].detach().cpu().numpy()\n",
        "      gnn_embedding = gnn_embedding1.flatten()\n",
        "      print(f\"GNN embedding shape: {gnn_embedding.shape}\")\n",
        "\n",
        "\n",
        "      lstm_input_data = transaction[['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].astype(float).fillna(0).values\n",
        "\n",
        "      lstm_input = torch.tensor(lstm_input_data.reshape(1, 1, -1),\n",
        "                                dtype=torch.float32)\n",
        "      lstm_score = self.lstm_model(lstm_input).detach().cpu().numpy().flatten()\n",
        "      print(f\"LSTM score shape: {lstm_score.shape}\")\n",
        "\n",
        "      feature_vector = np.concatenate((gnn_embedding, lstm_score))\n",
        "\n",
        "      print(f\"Final feature vector shape: {feature_vector.shape}\")\n",
        "\n",
        "      if feature_vector.shape != (65,):\n",
        "        raise ValueError(f\"Feature vector shape mismatch: Expected (65,), got{feature_vector.shape}\")\n",
        "\n",
        "      return feature_vector\n",
        "\n",
        "    def step(self, action):\n",
        "        transaction = self.df.iloc[self.current_index]\n",
        "\n",
        "\n",
        "        lstm_input = torch.tensor([[transaction[['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].values]], dtype=torch.float32)\n",
        "        anomaly_score = self.lstm_model(lstm_input).detach().cpu().numpy().item()\n",
        "\n",
        "        reward = 0\n",
        "        if anomaly_score > 0.5:  # High anomaly score → likely fraud\n",
        "            if action == 1: reward = 10  # Flagging an anomaly is good\n",
        "            elif action == 2: reward = 15  # Blocking an anomaly is best\n",
        "            else: reward = -10  # Ignoring an anomaly is bad\n",
        "        else:\n",
        "            if action == 1: reward = -5  # False positive penalty\n",
        "            elif action == 2: reward = -10  # Blocking legit transactions is bad\n",
        "            else: reward = 5  # Ignoring normal transaction is good\n",
        "\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= len(self.df) - 1\n",
        "        next_state = self._get_features(self.df.iloc[self.current_index]) if not done else np.zeros(65)\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_lstm_fraud_scores(df, lstm_model):\n",
        "    fraud_scores = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        lstm_input_data = df.iloc[i][['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].astype(float).values\n",
        "        lstm_input = torch.tensor(lstm_input_data.reshape(1, 1, -1), dtype=torch.float32)\n",
        "        score = lstm_model(lstm_input).detach().cpu().numpy().flatten()[0]\n",
        "        fraud_scores.append(score)\n",
        "\n",
        "    # ✅ Ensure fraud_scores has the same length as df before adding to DataFrame\n",
        "    df['Fraud_Score'] = fraud_scores\n",
        "    # Plot using seaborn\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.histplot(df['Fraud_Score'], bins=50, kde=True)\n",
        "    plt.xlabel(\"LSTM Fraud Score\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Distribution of Fraud Scores from LSTM\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_gnn_embeddings(df, lstm_model, gnn_model):\n",
        "    fraud_scores = []\n",
        "    for i in range(len(df)):\n",
        "      lstm_input_data = df.iloc[i][['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].astype(float).values\n",
        "      lstm_input = torch.tensor(lstm_input_data.reshape(1, 1, -1), dtype=torch.float32)\n",
        "      score = lstm_model(lstm_input).detach().cpu().numpy().flatten()[0]\n",
        "      fraud_scores.append(score)\n",
        "\n",
        "    node_features = torch.tensor(df[['Amount', 'Gas_Price', 'Gas_Used', 'Sender_Receiver_Count']].astype(float).values,\n",
        "      dtype=torch.float32)\n",
        "    unique_addresses = pd.unique(df[['Sender', 'Receiver']].values.ravel())\n",
        "    address_to_idx = {addr: idx for idx, addr in enumerate(unique_addresses)}\n",
        "\n",
        "    df['Sender_idx'] = df['Sender'].map(address_to_idx)\n",
        "    df['Receiver_idx'] = df['Receiver'].map(address_to_idx)\n",
        "\n",
        "    df[['Sender_idx', 'Receiver_idx']] = df[['Sender_idx', 'Receiver_idx']].fillna(-1).astype(int)\n",
        "\n",
        "    edge_index = torch.tensor(df[['Sender_idx', 'Receiver_idx'\n",
        "                            ]].values.T, dtype=torch.long)\n",
        "\n",
        "    node_features = torch.tensor(df[['Amount', 'Gas_Price', 'Gas_Used','Sender_Receiver_Count']].values, dtype=torch.float32)\n",
        "\n",
        "    embeddings = gnn_model.get_node_embeddings(node_features, edge_index)\n",
        "    embeddings = embeddings.detach().cpu().numpy()\n",
        "    df['Fraud_Score'] = fraud_scores\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=embeddings[:, 0], y=embeddings[:, 1], hue=df['Fraud_Score'], palette=\"coolwarm\", alpha=0.7)\n",
        "    plt.xlabel(\"GNN Embedding Dimension 1\")\n",
        "    plt.ylabel(\"GNN Embedding Dimension 2\")\n",
        "    plt.title(\"GNN Node Embeddings Visualization\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_lstm_fraud_scores(df, lstm_model)\n",
        "plot_gnn_embeddings(df,lstm_model, gnn_model)\n",
        "\n",
        "# After running RL agent\n",
        "actions = []\n",
        "env_instance = FraudDetectionEnv(df, gnn_model, lstm_model)\n",
        "vec_env = DummyVecEnv([lambda: env_instance])\n",
        "ppo_model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "ppo_model.learn(total_timesteps=10000)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    features = env_instance._get_features(df.iloc[i])\n",
        "    action, _ = ppo_model.predict(features)\n",
        "    actions.append(action)\n",
        "\n",
        "actions = np.array(actions)\n",
        "\n",
        "def plot_rl_agent_decisions(actions):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    sns.histplot(actions, bins=3, discrete=True, kde=False, color=\"blue\")\n",
        "    plt.xticks([0, 1, 2], [\"Ignore\", \"Flag\", \"Block\"])\n",
        "    plt.title(\"RL Agent Actions Distribution\")\n",
        "    plt.xlabel(\"Action\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "plot_rl_agent_decisions(actions)\n",
        "\n",
        "print(f\"RL Decision: {['Ignore', 'Flag', 'Block'][action]}\")\n",
        "\n",
        "\n",
        "# choose the synthetic_eth_transactions (1).csv file while uploading"
      ],
      "metadata": {
        "id": "Y43uRur8fQ6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Autoencoder Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JHUHz2ibf-yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['anomaly_score'], bins=30, kde=True)\n",
        "plt.axvline(df['anomaly_score'].quantile(0.99), color='r', linestyle='dashed', label='99th Percentile Threshold')\n",
        "plt.title('Anomaly Score Distribution')\n",
        "plt.xlabel('Anomaly Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z9y4QKIJf--I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# K-Means Visualization\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=df['kmeans_cluster'], palette='viridis')\n",
        "plt.title(\"K-Means Clustering\")\n",
        "\n",
        "# DBSCAN Visualization\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=df['dbscan_cluster'], palette='coolwarm')\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "y4HudssQgHMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# Initialize SHAP Explainer\n",
        "explainer = shap.TreeExplainer(iso_forest)\n",
        "shap_values = explainer.shap_values(scaled_data)\n",
        "\n",
        "# Compute Mean Absolute SHAP Value for Each Feature\n",
        "shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "# Sort Feature Importances\n",
        "sorted_indices = np.argsort(shap_importance)[::-1]\n",
        "\n",
        "# Plot Feature Importance\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=[numeric_features.columns[i] for i in sorted_indices], y=shap_importance[sorted_indices])\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Feature Importance (SHAP) - Isolation Forest\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Mean SHAP Value\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ-0FU8PgHbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_tx = df[df['fraud_flag'] == 1].head(50)  # Get top fraud cases\n",
        "\n",
        "G_sub = nx.DiGraph()\n",
        "for _, row in fraud_tx.iterrows():\n",
        "    G_sub.add_edge(row['sender'], row['receiver'], weight=row['amount'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "nx.draw(G_sub, with_labels=True, node_color='red', edge_color='gray', node_size=500)\n",
        "plt.title(\"Fraudulent Transaction Subgraph\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gfIllxqZj0tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "Wh8ViU5AkcOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (first 200 rows)\n",
        "\n",
        "df = pd.read_csv('/content/synthetic_eth_transactions.csv', nrows=200)\n",
        "\n",
        "\n",
        "\n",
        "# Function to compress Ethereum addresses (first and last character only)\n",
        "def compress_address(address):\n",
        "    return f\"{address[0]}...{address[-1]}\"\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges (transactions) to the graph\n",
        "for _, row in df.iterrows():\n",
        "    sender = compress_address(str(row['sender']))\n",
        "    receiver = compress_address(str(row['receiver']))\n",
        "    amount = row['amount']\n",
        "    G.add_edge(sender, receiver, weight=amount)\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "pos = nx.spring_layout(G, seed=42)  # Fixed layout for consistent visualization\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, edge_color='gray', width=2, font_size=10, font_weight='bold')\n",
        "\n",
        "# Draw edge labels (transaction amounts)\n",
        "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
        "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
        "\n",
        "# Show the graph\n",
        "plt.title(\"Blockchain Transaction Graph (First 200 Rows)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pfO-kKFHfuEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Qg4TjgTWa7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(model, \"fraud_model.joblib\")\n"
      ],
      "metadata": {
        "id": "jAKrPye0Nv9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxmltools\n",
        "onnx_model = onnxmltools.convert_sklearn(model)\n",
        "onnxmltools.utils.save_model(onnx_model, \"fraud_model.onnx\")\n"
      ],
      "metadata": {
        "id": "Bb0WBqqWRuMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "model = joblib.load(\"fraud_model.joblib\")\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(data: list):\n",
        "    prediction = model.predict(np.array(data).reshape(1, -1))\n",
        "    return {\"fraudulent\": bool(prediction[0])}\n"
      ],
      "metadata": {
        "id": "mh_mPxJrRuPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}